{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Noa Hen - 207422221\n",
    "Romi Pitshon - 314919614"
   ],
   "metadata": {
    "id": "ky6-ypHMUNRW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "065nO1X5CnNZ",
    "outputId": "4dec29ee-6992-4c6d-c6da-c709583984fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\romip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\romip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\romip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('spam_ham_dataset.csv')"
   ],
   "metadata": {
    "id": "2W7ILpjeC0kd"
   },
   "execution_count": 274,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['label'].value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_ChyXvrDTwS",
    "outputId": "264bf9c3-8a40-46a5-d16f-6ccd63d88b52"
   },
   "execution_count": 275,
   "outputs": [
    {
     "data": {
      "text/plain": "ham     3672\nspam    1499\nName: label, dtype: int64"
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Creating variables to perform tokenization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ],
   "metadata": {
    "id": "CZwxZCLbGi5f"
   },
   "execution_count": 276,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#A function that receives the email messages from the file and turns it into a list of tokens\n",
    "def message_to_token_list(s):\n",
    "  tokens = tokenizer.tokenize(s)\n",
    "  lowercased_tokens = [t.lower() for t in tokens]\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]\n",
    "  useful_tokens = [t for t in lemmatized_tokens if t not in stop_words]\n",
    "\n",
    "  return useful_tokens"
   ],
   "metadata": {
    "id": "Q36C2pbNDZav"
   },
   "execution_count": 277,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#split the data to train set and test set\n",
    "df = df.sample(frac=1, random_state=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df, test_df = df[:split_index], df[split_index:]\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOfPYpV7EHKX",
    "outputId": "f217d1d8-7433-4832-a173-2740fcbeffab"
   },
   "execution_count": 278,
   "outputs": [
    {
     "data": {
      "text/plain": "(      Unnamed: 0 label                                               text  \\\n 0           3430   ham  Subject: what the heck\\r\\ndaren ,\\r\\nnow what ...   \n 1           2070   ham  Subject: hilcorp old ocean volume\\r\\naccording...   \n 2           3974  spam  Subject: jurirne get latest softwares , 99 % s...   \n 3           1502   ham  Subject: sitara patch\\r\\na patch is being rele...   \n 4           4591  spam  Subject: archived great shots of california li...   \n ...          ...   ...                                                ...   \n 4131        4083  spam  Subject: viewsonic airpanel vl 50 15 - inch sm...   \n 4132        1297   ham  Subject: fw : why you shouldn ' t piss on the ...   \n 4133        2057   ham  Subject: holiday invitation\\r\\nplease click on...   \n 4134        2398   ham  Subject: managing director and vice president ...   \n 4135         805   ham  Subject: eastrans nom - 5 / 24 / 2000\\r\\nthis ...   \n \n       label_num  \n 0             0  \n 1             0  \n 2             1  \n 3             0  \n 4             1  \n ...         ...  \n 4131          1  \n 4132          0  \n 4133          0  \n 4134          0  \n 4135          0  \n \n [4136 rows x 4 columns],\n       Unnamed: 0 label                                               text  \\\n 0            253   ham  Subject: re :\\r\\ndaren ,\\r\\njust call me mike ...   \n 1            992   ham  Subject: enron / hpl actuals for june 15 , 200...   \n 2           4464  spam  Subject: anouncing a new player in the market ...   \n 3           1108   ham  Subject: re : republic royalty 5 / 00\\r\\ndone ...   \n 4           4816  spam  Subject: instructions to remove spyware / adwa...   \n ...          ...   ...                                                ...   \n 1030        3739  spam  Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\n...   \n 1031         374   ham  Subject: 29 th changes\\r\\n- - - - - - - - - - ...   \n 1032        4326  spam  Subject: buy regalis , also known as superviag...   \n 1033         803   ham  Subject: 24 x 5 products\\r\\nplease make sure t...   \n 1034         256   ham  Subject: 5 th changes @ duke and air liquide\\r...   \n \n       label_num  \n 0             0  \n 1             0  \n 2             1  \n 3             0  \n 4             1  \n ...         ...  \n 1030          1  \n 1031          0  \n 1032          1  \n 1033          0  \n 1034          0  \n \n [1035 rows x 4 columns])"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Performing an EDA that samples the information and shows the frequency of tokens for each category\n",
    "token_counter = {}\n",
    "\n",
    "for message in train_df['text']:\n",
    "  message_as_token_lst = message_to_token_list(message)\n",
    "\n",
    "  for token in message_as_token_lst:\n",
    "    if token in token_counter:\n",
    "      token_counter[token] += 1\n",
    "    else:\n",
    "      token_counter[token] = 1\n",
    "\n",
    "len(token_counter)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-P_KzmyPFcKe",
    "outputId": "68dd8ecf-2158-40d0-9110-40e2f3357bd2"
   },
   "execution_count": 279,
   "outputs": [
    {
     "data": {
      "text/plain": "42460"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "token_counter"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiyuG12TLg7Y",
    "outputId": "8cd5b267-4a90-4726-f595-86d69f4abf62"
   },
   "execution_count": 280,
   "outputs": [
    {
     "data": {
      "text/plain": "{'subject': 6425,\n 'heck': 2,\n 'daren': 1460,\n 'see': 973,\n 'ken': 101,\n 'back': 488,\n 'jeff': 102,\n 'development': 206,\n 'ha': 1394,\n 'lot': 95,\n 'bud': 4,\n 'high': 305,\n 'place': 244,\n 'could': 447,\n 'many': 244,\n 'change': 923,\n 'horizon': 3,\n 'moved': 73,\n 'new': 1170,\n 'building': 47,\n 'yet': 124,\n 'looking': 302,\n 'tour': 24,\n 'invitation': 46,\n 'kid': 27,\n 'wife': 54,\n 'home': 272,\n 'read': 174,\n 'work': 445,\n 'project': 191,\n 'jen': 5,\n 'couple': 65,\n 'week': 430,\n 'heading': 9,\n 'waco': 6,\n 'going': 238,\n 'law': 47,\n 'school': 64,\n 'built': 30,\n 'tobacco': 7,\n 'settlement': 122,\n 'money': 299,\n 'feel': 157,\n 'like': 533,\n 'contributators': 1,\n 'four': 72,\n 'quarter': 59,\n '5': 1256,\n 'maybe': 43,\n '6': 857,\n 'go': 440,\n 'son': 25,\n 'still': 251,\n 'florida': 31,\n 'plan': 242,\n 'run': 94,\n 'september': 248,\n 'day': 1239,\n 'disneyworld': 1,\n 'jeni': 1,\n 'supposed': 14,\n 'get': 1058,\n 'u': 1096,\n 'ticket': 663,\n 'nebraska': 3,\n 'game': 80,\n 'year': 416,\n 'fun': 59,\n 'speaking': 12,\n 'football': 11,\n 'one': 787,\n 'texas': 657,\n 'team': 272,\n 'poll': 1,\n 'got': 145,\n 'toy': 7,\n 'sears': 10,\n 'good': 247,\n 'sale': 803,\n 'power': 178,\n 'washer': 1,\n 'eye': 53,\n 'bought': 47,\n 'patio': 1,\n 'deck': 6,\n 'god': 79,\n 'thought': 141,\n 'painting': 4,\n 'wa': 1357,\n 'mindless': 1,\n 'also': 616,\n 'found': 102,\n 'insurance': 22,\n 'company': 1200,\n 'funding': 14,\n 'entirely': 8,\n 'roof': 10,\n 'hail': 5,\n 'damage': 13,\n 'mess': 8,\n 'old': 135,\n 'layover': 1,\n 'shake': 9,\n 'shingle': 2,\n 'glad': 23,\n 'rid': 9,\n 'decking': 1,\n 'know': 1284,\n 'busy': 20,\n 'let': 942,\n 'latest': 88,\n 'time': 881,\n 'later': 84,\n 'hilcorp': 17,\n 'ocean': 42,\n 'volume': 1323,\n 'according': 73,\n 'gary': 449,\n 'hank': 179,\n 'would': 853,\n 'gas': 2431,\n 'valley': 135,\n 'line': 328,\n 'anyway': 27,\n 'forwarded': 1041,\n 'mary': 438,\n 'jo': 52,\n 'johnson': 68,\n 'hou': 5921,\n 'ect': 11309,\n '11': 1214,\n '20': 747,\n '2000': 3580,\n '03': 987,\n '26': 346,\n 'pm': 1885,\n 'jill': 56,\n 'zivley': 39,\n 'cc': 1883,\n 'edward': 105,\n 'gottlob': 42,\n 'lauri': 118,\n 'allen': 165,\n 'producer': 130,\n 'service': 652,\n 'group': 376,\n 'believe': 178,\n 'sending': 42,\n 'email': 718,\n 'form': 171,\n 'hawaii': 9,\n 'crazy': 12,\n 'took': 75,\n 'care': 72,\n 'camden': 18,\n 'month': 876,\n '330': 23,\n 'responded': 7,\n 'spoke': 81,\n 'jerry': 26,\n 'bubert': 7,\n 'several': 101,\n 'today': 432,\n 'emailed': 4,\n 'mike': 156,\n 'lannou': 176,\n 'left': 109,\n 'message': 821,\n 'paged': 6,\n 'copied': 20,\n 'hildebrand': 3,\n 'president': 114,\n 'hear': 67,\n '4': 1262,\n 'bypass': 14,\n 'jurirne': 1,\n 'software': 210,\n '99': 806,\n 'saving': 84,\n 'cqunecc': 1,\n 'microsoft': 193,\n 'window': 239,\n 'xp': 152,\n 'professional': 191,\n '50': 362,\n 'adobe': 151,\n 'photoshop': 74,\n '7': 883,\n '0': 1315,\n '60': 265,\n 'office': 370,\n 'pagemaker': 14,\n 'illustrator': 31,\n '10': 1685,\n 'corel': 62,\n 'draw': 54,\n 'graphic': 42,\n 'suite': 131,\n 'visual': 19,\n 'studio': 35,\n 'net': 591,\n 'norton': 34,\n 'antivirus': 18,\n 'corporate': 68,\n 'edition': 63,\n '2003': 137,\n '40': 295,\n 'system': 536,\n 'deluxe': 7,\n 'delphi': 11,\n 'sql': 17,\n 'server': 132,\n 'enterprise': 57,\n 'architect': 16,\n '120': 126,\n 'welcome': 54,\n 'sitara': 667,\n 'patch': 28,\n 'released': 29,\n 'resolve': 48,\n 'problem': 287,\n 'update': 191,\n 'prior': 198,\n 'position': 209,\n 'manager': 211,\n 'related': 90,\n 'feedback': 108,\n 'held': 61,\n 'queue': 4,\n 'evening': 39,\n 'please': 2576,\n 'call': 649,\n 'cpr': 44,\n 'hotline': 16,\n '713': 468,\n '853': 75,\n '7049': 3,\n 'question': 705,\n 'release': 154,\n 'archived': 1,\n 'great': 193,\n 'shot': 23,\n 'california': 69,\n 'living': 23,\n 'greeting': 8,\n 'love': 130,\n 'favourite': 2,\n 'celebs': 2,\n 'revolting': 2,\n 'jpeg': 3,\n 'madonna': 3,\n 'link': 263,\n 'sexy': 9,\n 'avi': 3,\n 'sarah': 16,\n 'michelle': 13,\n 'gellar': 1,\n 'groundbreaking': 2,\n 'identity': 16,\n 'theft': 4,\n 'pamela': 9,\n 'anderson': 32,\n 'heartbreaking': 1,\n 'picture': 64,\n 'mariah': 1,\n 'carey': 10,\n 'remove': 154,\n 'http': 956,\n 'www': 585,\n '24': 447,\n 'medz': 7,\n 'com': 3214,\n 'rms': 3,\n 'html': 111,\n 'bankofamerica': 3,\n 'account': 334,\n 'quinn': 5,\n 'hamlin': 2,\n 'chandler': 4,\n 'bioniche': 1,\n 'india': 13,\n 'bangalore': 2,\n '560085': 1,\n 'phone': 294,\n '946': 1,\n '776': 4,\n '4467': 1,\n 'mobile': 96,\n '632': 3,\n '797': 2,\n '5521': 1,\n 'fywsoobtmvj': 1,\n 'future': 267,\n 'online': 270,\n 'auto': 33,\n 'generated': 26,\n 'reply': 188,\n 'version': 104,\n '01': 1436,\n 'decade': 13,\n 'definite': 5,\n 'download': 100,\n 'note': 285,\n 'content': 169,\n 'info': 233,\n 'manipulation': 5,\n 'incestuous': 1,\n 'isadore': 2,\n 'cuba': 5,\n 'transferred': 19,\n 'alexandria': 1,\n 'mon': 15,\n 'dec': 185,\n '2004': 280,\n '07': 523,\n '17': 350,\n '0200': 20,\n 'hillarious': 1,\n 'take': 460,\n 'minute': 83,\n '800': 134,\n '578': 6,\n '7453': 1,\n 'listen': 13,\n 'brown': 54,\n 'williamson': 6,\n 'point': 346,\n 'asked': 78,\n 'select': 67,\n 'extension': 31,\n 'hang': 7,\n 'david': 262,\n 'heineke': 4,\n 'dheineke': 5,\n 'tsteel': 5,\n 'devon': 98,\n 'understand': 65,\n 'guy': 92,\n 'want': 542,\n 'talk': 84,\n 'hmmmmmmmmm': 1,\n 'tomorrow': 120,\n 'thursday': 194,\n '8': 734,\n 'th': 697,\n '3': 2457,\n 'sometime': 21,\n 'thanks': 1511,\n 'feb': 232,\n 'noms': 277,\n 'ami': 310,\n 'chokshi': 235,\n 'corp': 1419,\n 'enron': 5322,\n '08': 637,\n '31': 496,\n 'troy': 53,\n '_': 2513,\n 'benoit': 29,\n 'reliantenergy': 67,\n '23': 369,\n '47': 102,\n 'attached': 891,\n 'file': 608,\n 'hpl': 1819,\n 'xl': 856,\n 'important': 99,\n 'information': 951,\n 'participant': 37,\n 'due': 314,\n 'current': 215,\n 'business': 543,\n 'circumstance': 9,\n 'november': 241,\n '28': 382,\n '2001': 1611,\n 'board': 46,\n 'director': 111,\n 'adopted': 7,\n 'amendment': 7,\n 'includes': 74,\n 'following': 527,\n 'became': 13,\n 'effective': 408,\n '29': 325,\n 'contribution': 36,\n 'deducted': 4,\n '30': 745,\n 'paycheck': 14,\n 'matched': 3,\n 'cash': 124,\n 'rather': 36,\n 'stock': 460,\n 'match': 43,\n 'placed': 52,\n 'fidelity': 3,\n 'freedom': 17,\n 'fund': 107,\n 'default': 20,\n 'longer': 82,\n 'exception': 91,\n 'continue': 200,\n 'union': 94,\n 'matching': 17,\n 'always': 122,\n 'permitted': 10,\n 'employee': 217,\n 'alternative': 43,\n 'investment': 309,\n 'option': 167,\n 'immediately': 84,\n 'diversification': 2,\n 'within': 368,\n 'allowed': 23,\n 'regardless': 21,\n 'age': 51,\n 'restriction': 18,\n 'diversify': 3,\n 'include': 152,\n 'qnec': 2,\n 'move': 153,\n 'web': 177,\n 'choose': 67,\n 'restricted': 32,\n 'employer': 10,\n 'acct': 2,\n 'transfer': 105,\n 'reallocation': 3,\n 'drop': 32,\n 'menu': 18,\n 'voice': 77,\n 'response': 99,\n 'request': 329,\n 'allow': 87,\n 'formerly': 21,\n 'avoid': 45,\n 'delay': 32,\n 'recommend': 14,\n 'access': 185,\n 'resource': 261,\n 'hewitt': 7,\n 'benefit': 77,\n 'received': 199,\n '182': 2,\n '152': 6,\n '35': 140,\n '180': 27,\n '220': 26,\n '91': 22,\n '41': 81,\n '100': 414,\n 'tue': 17,\n 'sep': 17,\n '15': 549,\n '59': 90,\n '39': 103,\n 'id': 191,\n 'ilbnhtgbuxpdyk': 1,\n 'hushmail': 1,\n 'hector': 33,\n 'guevara': 3,\n 'zipusx': 2,\n 'forsuccessnow': 2,\n 'paliourg': 93,\n 'iit': 49,\n 'demokritos': 49,\n 'gr': 53,\n 'tired': 17,\n 'broke': 3,\n 'e': 1639,\n 'date': 316,\n '0400': 6,\n 'mime': 18,\n '1': 2209,\n 'type': 167,\n 'multipart': 12,\n 'boundary': 9,\n '86379931627997882': 3,\n 'rnd': 52,\n 'space': 26,\n 'priority': 21,\n 'text': 79,\n 'encoding': 13,\n 'bit': 45,\n 'bitnum': 1,\n 'doctype': 1,\n 'public': 86,\n 'w': 349,\n 'c': 604,\n 'dtd': 1,\n 'transitional': 1,\n 'en': 35,\n 'htmlhead': 2,\n 'meta': 9,\n 'equiv': 7,\n 'dcontent': 2,\n 'dtext': 2,\n 'charset': 32,\n 'dwindows': 2,\n '12': 1089,\n '52': 110,\n 'dmshtml': 2,\n '00': 1962,\n '2800': 4,\n '1458': 1,\n 'name': 393,\n 'dgenerator': 2,\n 'style': 40,\n 'head': 104,\n 'body': 68,\n 'bgcolor': 81,\n 'ffffff': 27,\n 'divnbsp': 1,\n 'div': 66,\n 'center': 204,\n 'table': 71,\n 'dwidth': 2,\n '586': 3,\n 'px': 14,\n 'height': 151,\n '544': 5,\n 'cellspacing': 26,\n 'cellpadding': 36,\n 'width': 189,\n '610': 4,\n 'background': 18,\n 'dhttp': 10,\n 'mynetmarketer': 1,\n 'image': 165,\n 'clrwaterl': 1,\n 'jpg': 73,\n 'border': 135,\n 'dl': 25,\n 'tbody': 10,\n 'tr': 146,\n 'td': 272,\n 'valign': 29,\n 'dtop': 3,\n '534': 5,\n 'nbsp': 369,\n 'align': 180,\n 'dcenter': 16,\n '555': 2,\n '297': 7,\n 'cellpaddi': 1,\n 'ng': 20,\n '000000': 59,\n '322': 20,\n 'font': 301,\n 'color': 178,\n 'fffbf': 11,\n 'strongfont': 4,\n 'size': 214,\n '2': 1937,\n 'p': 702,\n 'dcenterbr': 1,\n 'fontfont': 43,\n 'face': 172,\n 'darialnbsp': 2,\n 'marketing': 193,\n 'effort': 107,\n 'vain': 1,\n 'dcenternbsp': 2,\n 'dlimg': 1,\n '84': 45,\n 'worried': 6,\n 'sm': 19,\n 'wht': 2,\n 'gif': 60,\n 'dcenterfont': 18,\n 'darialtired': 2,\n 'calling': 51,\n 'zero': 147,\n 'result': 210,\n 'dcenterimg': 1,\n '55': 94,\n 'wasting': 8,\n 'sub': 9,\n 'mitting': 1,\n 'safe': 58,\n 'list': 481,\n 'ezines': 1,\n 'classified': 1,\n 'fontbrimg': 1,\n '75': 88,\n 'dobrfont': 1,\n 'every': 181,\n 'darial': 50,\n 'emstrongfont': 1,\n 'ffffo': 1,\n '02': 824,\n 'top': 146,\n 'quality': 141,\n 'product': 517,\n 'fontnbsp': 1,\n '0000': 16,\n 'ff': 17,\n '98': 235,\n 'mass': 15,\n 'exposurenbsp': 1,\n 'strong': 103,\n 'em': 24,\n 'available': 317,\n 'stron': 1,\n 'g': 245,\n 'dcenterstrongfont': 5,\n 'ffo': 15,\n 'emanywhere': 1,\n 'else': 97,\n 'ema': 5,\n 'ffffffstrongfont': 22,\n 'darialcurrently': 1,\n 'industry': 137,\n 'failure': 21,\n 'rate': 384,\n 'arialwith': 1,\n 'marketer': 13,\n 'experience': 79,\n 'success': 60,\n 'ize': 4,\n 'matter': 88,\n 'l': 1011,\n 'earn': 11,\n 'master': 37,\n 'target': 87,\n 'strongstrongfont': 1,\n 'da': 41,\n 'rial': 1,\n 'brgrow': 1,\n 'thousand': 93,\n 'automatically': 34,\n 'brearn': 1,\n '000': 1722,\n 'first': 428,\n 'brlearn': 2,\n 'attraction': 9,\n 'selling': 55,\n 'technique': 13,\n 'elite': 5,\n 'ake': 1,\n 'profit': 64,\n 'darialimg': 1,\n 'ufont': 1,\n 'strongno': 1,\n 'necessary': 117,\n 'unbsp': 1,\n 'stronganyone': 1,\n 'determined': 25,\n 'make': 496,\n 'per': 471,\n 'bron': 1,\n 'intern': 7,\n 'et': 28,\n 'img': 48,\n 'heig': 1,\n 'ht': 4,\n 'ill': 11,\n 'md': 9,\n 'strongemnbsp': 1,\n 'member': 110,\n 'ofnbsp': 1,\n 'thebrmost': 1,\n 'powerful': 22,\n 'internet': 223,\n 'brand': 94,\n 'help': 420,\n 'expert': 32,\n 'build': 43,\n 'brnbsp': 2,\n 'exclusivenbsp': 1,\n 'tobrtemplates': 1,\n 'animation': 1,\n 'flash': 98,\n 'presentation': 45,\n 'much': 196,\n 'emthis': 1,\n 'ferrari': 2,\n 'br': 153,\n 'dleftstrongemfont': 6,\n 'fffb': 2,\n 'f': 323,\n 'ffffffdont': 1,\n 'fooled': 5,\n 'worthlessnbsp': 1,\n 'fresh': 16,\n 'lead': 62,\n 'strongstrongemfont': 5,\n 'ffffffnbsp': 5,\n 'ffffffwho': 1,\n 'actually': 42,\n 'deliver': 49,\n 'contact': 511,\n 'data': 214,\n 'base': 93,\n 'dari': 4,\n 'al': 104,\n 'someone': 167,\n 'wh': 3,\n 'viewed': 6,\n 'offering': 70,\n 'ffffffresponded': 1,\n 'real': 117,\n 'systemnbsp': 1,\n 'teach': 11,\n 'fffffyou': 1,\n 'generation': 45,\n 'expertbr': 1,\n 'dcenterif': 1,\n 'best': 296,\n 'emfont': 1,\n 'href': 123,\n '211': 6,\n '158': 7,\n 'hg': 2,\n 'free': 408,\n 'stronghector': 1,\n '2411': 1,\n 'cleveland': 4,\n 'street': 41,\n '183': 7,\n 'clearwater': 3,\n '33755': 1,\n 'arial': 18,\n 'strongoffice': 1,\n '801': 8,\n '693': 9,\n '5350': 1,\n 'cell': 60,\n '631': 4,\n '834': 2,\n '1207': 1,\n 'di': 5,\n 'v': 237,\n 'centerfont': 11,\n 'may': 1153,\n 'un': 31,\n 'subscribe': 11,\n 'fonta': 1,\n '66': 24,\n '78': 40,\n '254': 12,\n 'cgi': 9,\n 'bin': 12,\n 'rem': 12,\n 'myr': 2,\n 'dir': 14,\n 'dmsufont': 1,\n 'afont': 2,\n 'inbound': 5,\n 'quarantined': 2,\n 'attempted': 7,\n 'send': 304,\n 'mail': 781,\n 'outside': 36,\n 'attachment': 62,\n 'doe': 249,\n 'messaging': 23,\n 'environment': 64,\n 'mailsweeper': 1,\n 'sender': 28,\n 'colio': 1,\n 'houston': 385,\n 'rr': 25,\n 'thu': 111,\n '27': 376,\n '16': 423,\n '0500': 16,\n 'logitech': 5,\n 'video': 79,\n 'scenario': 10,\n 'incoming': 3,\n 'exe': 12,\n 'catcher': 1,\n 'filename': 7,\n 'mask': 1,\n 'detected': 4,\n 'vmailvid': 1,\n 'intended': 104,\n 'valid': 129,\n 'requires': 19,\n 'retrieve': 12,\n 'desk': 304,\n 'ask': 87,\n 'quarantine': 2,\n 'delivered': 81,\n 'inbox': 5,\n 'scanned': 2,\n 'checked': 34,\n 'virus': 19,\n 'requested': 117,\n 'contains': 57,\n 'reason': 115,\n 'suspect': 10,\n 'malicious': 4,\n 'code': 93,\n 'north': 333,\n 'american': 66,\n 'resolution': 28,\n '1411': 7,\n 'european': 31,\n '0044': 1,\n '207': 12,\n '36777': 1,\n 'ee': 170,\n '888': 20,\n '9797': 13,\n 'ets': 12,\n 'solution': 118,\n '345': 69,\n '4745': 2,\n '402': 2,\n '398': 5,\n '7454': 1,\n 'omaha': 6,\n 'address': 308,\n 'monitored': 2,\n 'mailbox': 26,\n 'duve': 4,\n 'khumalo': 5,\n 'tel': 64,\n '73': 35,\n '230': 11,\n '82': 23,\n 'duvmalo': 1,\n 'mailasia': 1,\n 'attn': 9,\n 'dear': 80,\n 'respect': 60,\n 'trust': 22,\n 'humility': 1,\n 'use': 379,\n 'medium': 84,\n 'write': 83,\n 'letter': 142,\n 'irrespective': 1,\n 'fact': 159,\n 'consider': 27,\n 'family': 82,\n 'dare': 4,\n 'need': 1447,\n 'assistance': 82,\n 'formally': 4,\n 'introduce': 17,\n 'late': 95,\n 'dr': 42,\n 'robinson': 15,\n 'renowned': 3,\n 'zimbabwean': 3,\n 'wine': 5,\n 'cattle': 6,\n 'ranch': 109,\n 'farmer': 878,\n 'courtesy': 9,\n 'journal': 17,\n 'south': 104,\n 'african': 4,\n 'exchange': 139,\n 'earnest': 7,\n 'search': 45,\n 'reliable': 30,\n 'trustworthy': 7,\n 'individual': 89,\n 'assist': 61,\n 'war': 22,\n 'waged': 1,\n 'zimbabwe': 13,\n 'supporter': 3,\n 'cohort': 2,\n 'robert': 699,\n 'mugabe': 3,\n 'claim': 102,\n 'white': 72,\n 'owned': 30,\n 'farm': 18,\n 'country': 105,\n 'father': 39,\n 'belonged': 1,\n 'class': 73,\n 'targeted': 12,\n 'zanu': 2,\n 'pf': 3,\n 'armed': 4,\n 'support': 194,\n 'idea': 76,\n 'policy': 57,\n 'course': 58,\n 'revolution': 2,\n 'attacked': 1,\n 'invaded': 2,\n 'burning': 10,\n 'destroying': 2,\n 'eventually': 10,\n 'killing': 23,\n 'death': 23,\n 'last': 353,\n 'managed': 12,\n 'escape': 3,\n 'africa': 29,\n 'mother': 18,\n 'life': 134,\n 'threatened': 2,\n 'escaped': 2,\n 'safely': 8,\n 'lifetime': 9,\n 'fortune': 25,\n '25': 546,\n 'twenty': 23,\n 'five': 63,\n 'million': 243,\n 'dollar': 179,\n 'bond': 4,\n 'document': 115,\n 'property': 123,\n 'title': 101,\n 'valuable': 26,\n 'instructed': 6,\n 'concealed': 1,\n 'secured': 19,\n 'two': 333,\n 'treasure': 8,\n 'box': 113,\n 'transported': 14,\n 'netherlands': 27,\n 'diplomatic': 5,\n 'mean': 76,\n 'currently': 249,\n 'vault': 5,\n 'private': 97,\n 'security': 413,\n 'brokerage': 9,\n 'firm': 161,\n 'presently': 45,\n 'residing': 1,\n 'temporarily': 9,\n 'johannesburg': 3,\n 'pending': 24,\n 'outcome': 5,\n 'appeal': 5,\n 'filed': 16,\n 'behalf': 41,\n 'attorney': 12,\n 'department': 95,\n 'affair': 13,\n 'grant': 37,\n 'political': 16,\n 'asylum': 7,\n 'moment': 29,\n 'dilemma': 2,\n 'moreover': 4,\n 'government': 62,\n 'stringent': 3,\n 'monetary': 2,\n 'regulation': 26,\n 'sensitive': 9,\n 'volatile': 14,\n 'status': 100,\n 'region': 52,\n 'dangerous': 6,\n 'attempt': 30,\n 'act': 190,\n 'jeopardize': 1,\n 'chance': 84,\n 'recognition': 5,\n 'personal': 106,\n 'executive': 54,\n 'opportunity': 113,\n 'abound': 2,\n 'solicit': 6,\n 'moving': 35,\n 'bank': 96,\n 'safekeeping': 3,\n 'arrival': 5,\n 'resolved': 44,\n 'compensate': 7,\n 'adequately': 2,\n 'commission': 25,\n 'earmarked': 1,\n 'pre': 41,\n 'expense': 125,\n 'incur': 9,\n 'transaction': 261,\n 'guarantee': 61,\n 'refund': 17,\n 'mind': 65,\n 'establish': 13,\n 'friendly': 15,\n 'relationship': 33,\n 'nearest': 4,\n 'able': 200,\n 'receive': 240,\n 'approval': 92,\n 'khumaduvel': 1,\n 'netscape': 10,\n 'number': 607,\n 'relevant': 17,\n 'kindly': 3,\n 'fax': 272,\n 'easier': 22,\n 'communication': 145,\n 'unable': 39,\n ...}"
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Selecting the features according to the threshold value\n",
    "def keep_token(proccessed_token, threshold):\n",
    "  if proccessed_token not in token_counter:\n",
    "    return False\n",
    "  else:\n",
    "    return token_counter[proccessed_token] > threshold"
   ],
   "metadata": {
    "id": "gBksrH3jMvu6"
   },
   "execution_count": 281,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "features = set()\n",
    "\n",
    "for token in token_counter:\n",
    "  if keep_token(token, 3000):\n",
    "    features.add(token)\n",
    "\n",
    "features"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__SOdaetMy3Z",
    "outputId": "1553ebc0-37e0-45b5-a260-ee2e808d9567"
   },
   "execution_count": 282,
   "outputs": [
    {
     "data": {
      "text/plain": "{'2000', 'com', 'ect', 'enron', 'hou', 'subject'}"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "features = list(features)\n",
    "features"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1Q_t9y9NBEr",
    "outputId": "ff24a72b-8681-42bf-b717-8bb004d44dad"
   },
   "execution_count": 283,
   "outputs": [
    {
     "data": {
      "text/plain": "['ect', 'hou', 'com', '2000', 'subject', 'enron']"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Giving a numerical value to each token in the list\n",
    "token_to_index_mapping = {t:i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-mnVItkN0t5",
    "outputId": "c8928a8a-fac3-4934-9d4e-b2fa15f55b68"
   },
   "execution_count": 284,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ect': 0, 'hou': 1, 'com': 2, '2000': 3, 'subject': 4, 'enron': 5}"
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#A function that receives an email message and counts the tokens that are in the list of features\n",
    "def message_to_count_vector(message):\n",
    "  count_vector = np.zeros(len(features))\n",
    "\n",
    "  processed_list_of_tokens = message_to_token_list(message)\n",
    "\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in features:\n",
    "      continue\n",
    "    index = token_to_index_mapping[token]\n",
    "    count_vector[index] += 1\n",
    "  \n",
    "  return count_vector"
   ],
   "metadata": {
    "id": "s_0qXyJEN0lG"
   },
   "execution_count": 285,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Division into a target column and a features column\n",
    "def df_to_X_y(dff):\n",
    "  y = dff['label_num'].to_numpy().astype(int)\n",
    "\n",
    "  message_col = dff['text']\n",
    "  count_vectors = []\n",
    "\n",
    "  for message in message_col:\n",
    "    count_vector = message_to_count_vector(message)\n",
    "    count_vectors.append(count_vector)\n",
    "\n",
    "  X = np.array(count_vectors).astype(int)\n",
    "\n",
    "  return X, y"
   ],
   "metadata": {
    "id": "54Ip7BjkPCKC"
   },
   "execution_count": 286,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "X_test, y_test = df_to_X_y(test_df)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N0funKqPzCW",
    "outputId": "582cd573-3d41-4820-a96d-58340de66ea2"
   },
   "execution_count": 287,
   "outputs": [
    {
     "data": {
      "text/plain": "((4136, 6), (4136,), (1035, 6), (1035,))"
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "X_train"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3U9omKpQWkl",
    "outputId": "0b4efbc9-c3a9-4f1f-c633-342ec728cfd4"
   },
   "execution_count": 288,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        ],\n       [0.02046784, 0.0239521 , 0.        , 0.03448276, 0.05263158,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.06896552, 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.00142653, 0.        , 0.        ,\n        0.0075188 ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.03448276, 0.        ,\n        0.0075188 ]])"
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Building a linear regression model from the scikit-learn library, predicting and performing an accuracy assessment for the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlpRV3O-RKaE",
    "outputId": "a90d8c3b-339f-4cf5-ea93-61db41c66bdc"
   },
   "execution_count": 289,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84       747\n",
      "           1       0.00      0.00      0.00       288\n",
      "\n",
      "    accuracy                           0.72      1035\n",
      "   macro avg       0.36      0.50      0.42      1035\n",
      "weighted avg       0.52      0.72      0.61      1035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romip\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\romip\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\romip\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "tpQwq_6ISOkw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90       747\n",
      "           1       0.89      0.49      0.63       288\n",
      "\n",
      "    accuracy                           0.84      1035\n",
      "   macro avg       0.86      0.73      0.77      1035\n",
      "weighted avg       0.85      0.84      0.82      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Repeating the prediction model in three additional runs with a different extraction of features in order to optimize the model\n",
    "features = set()\n",
    "\n",
    "for token in token_counter:\n",
    "  if keep_token(token, 1000):\n",
    "    features.add(token)\n",
    "\n",
    "features\n",
    "token_to_index_mapping = {t: i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping\n",
    "\n",
    "\n",
    "def message_to_count_vector(message):\n",
    "  count_vector = np.zeros(len(features))\n",
    "\n",
    "  processed_list_of_tokens = message_to_token_list(message)\n",
    "\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in features:\n",
    "      continue\n",
    "    index = token_to_index_mapping[token]\n",
    "    count_vector[index] += 1\n",
    "\n",
    "  return count_vector\n",
    "\n",
    "\n",
    "def df_to_X_y(dff):\n",
    "  y = dff['label_num'].to_numpy().astype(int)\n",
    "\n",
    "  message_col = dff['text']\n",
    "  count_vectors = []\n",
    "\n",
    "  for message in message_col:\n",
    "    count_vector = message_to_count_vector(message)\n",
    "    count_vectors.append(count_vector)\n",
    "\n",
    "  X = np.array(count_vectors).astype(int)\n",
    "\n",
    "  return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "X_test, y_test = df_to_X_y(test_df)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "X_train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94       747\n",
      "           1       0.84      0.83      0.83       288\n",
      "\n",
      "    accuracy                           0.91      1035\n",
      "   macro avg       0.89      0.88      0.88      1035\n",
      "weighted avg       0.91      0.91      0.91      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = set()\n",
    "\n",
    "for token in token_counter:\n",
    "  if keep_token(token, 800):\n",
    "    features.add(token)\n",
    "\n",
    "features\n",
    "token_to_index_mapping = {t: i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping\n",
    "\n",
    "\n",
    "def message_to_count_vector(message):\n",
    "  count_vector = np.zeros(len(features))\n",
    "\n",
    "  processed_list_of_tokens = message_to_token_list(message)\n",
    "\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in features:\n",
    "      continue\n",
    "    index = token_to_index_mapping[token]\n",
    "    count_vector[index] += 1\n",
    "\n",
    "  return count_vector\n",
    "\n",
    "\n",
    "def df_to_X_y(dff):\n",
    "  y = dff['label_num'].to_numpy().astype(int)\n",
    "\n",
    "  message_col = dff['text']\n",
    "  count_vectors = []\n",
    "\n",
    "  for message in message_col:\n",
    "    count_vector = message_to_count_vector(message)\n",
    "    count_vectors.append(count_vector)\n",
    "\n",
    "  X = np.array(count_vectors).astype(int)\n",
    "\n",
    "  return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "X_test, y_test = df_to_X_y(test_df)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "X_train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       747\n",
      "           1       0.95      0.93      0.94       288\n",
      "\n",
      "    accuracy                           0.97      1035\n",
      "   macro avg       0.96      0.96      0.96      1035\n",
      "weighted avg       0.97      0.97      0.97      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = set()\n",
    "\n",
    "for token in token_counter:\n",
    "  if keep_token(token, 50):\n",
    "    features.add(token)\n",
    "\n",
    "features\n",
    "token_to_index_mapping = {t: i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping\n",
    "\n",
    "\n",
    "def message_to_count_vector(message):\n",
    "  count_vector = np.zeros(len(features))\n",
    "\n",
    "  processed_list_of_tokens = message_to_token_list(message)\n",
    "\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in features:\n",
    "      continue\n",
    "    index = token_to_index_mapping[token]\n",
    "    count_vector[index] += 1\n",
    "\n",
    "  return count_vector\n",
    "\n",
    "\n",
    "def df_to_X_y(dff):\n",
    "  y = dff['label_num'].to_numpy().astype(int)\n",
    "\n",
    "  message_col = dff['text']\n",
    "  count_vectors = []\n",
    "\n",
    "  for message in message_col:\n",
    "    count_vector = message_to_count_vector(message)\n",
    "    count_vectors.append(count_vector)\n",
    "\n",
    "  X = np.array(count_vectors).astype(int)\n",
    "\n",
    "  return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "X_test, y_test = df_to_X_y(test_df)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "X_train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Features:    Accuracy:\n",
      "-----------  -----------\n",
      "       3000         0.72\n",
      "       1000         0.84\n",
      "        800         0.91\n",
      "         50         0.97\n"
     ]
    }
   ],
   "source": [
    "#A table showing each of the runs we performed with the amount of different features and comparing the results we received\n",
    "table = [['Features:', 'Accuracy:'], [3000, 0.72], [1000, 0.84], [800, 0.91], [50, 0.97]]\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "#Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on â€“ the kind of relationship between dependent and independent variables they are considering, and the number of independent variables getting used. As we decreased the number of features, the level of accuracy increased so that we were able to reach an accuracy level of 97% using 50 features. It can be understood that in our data, the more we reduced the number of features, the better the model was able to predict the relationship between the variables."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
